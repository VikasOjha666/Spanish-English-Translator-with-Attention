{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spanish to english translator with Attention Mechanism.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4VcqoUDLS93",
        "colab_type": "code",
        "outputId": "37cc05bc-9cf4-4e22-9b40-bf5919cf64ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Importing bunch of libraries and neural layers.\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "import unicodedata\n",
        "import re\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0rr_kOgLcDN",
        "colab_type": "code",
        "outputId": "2f25478c-2dff-4dd8-bfca-592ee9fb795d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-30 21:10:32--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4752884 (4.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.53M  12.2MB/s    in 0.4s    \n",
            "\n",
            "2019-12-30 21:10:38 (12.2 MB/s) - ‘spa-eng.zip’ saved [4752884/4752884]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDN8nsZoL1cY",
        "colab_type": "code",
        "outputId": "932ad279-d4fb-4b9f-eb6a-f73b59bd789b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!unzip spa-eng"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOTzmpE_cPO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        " \n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.rstrip().strip()\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNqHiqoQL6D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the dataset\n",
        "spa_sents=[]\n",
        "eng_sents=[]\n",
        "\n",
        "\n",
        "f=open('spa.txt','rt',encoding='utf-8')\n",
        "allsents=f.read()\n",
        "f.close()\n",
        "#Seperate the sents and form language pair.\n",
        "sents=allsents.strip().split('\\n')\n",
        "pair=[pr.split('\\t') for pr in sents]\n",
        "\n",
        "tempdf=pd.DataFrame(pair)\n",
        "lang_pairs=tempdf.iloc[:,:2].values\n",
        "\n",
        "\n",
        "#Removing punctuation.\n",
        "lang_pairs[:,0] = [(s.translate(str.maketrans('', '', string.punctuation))).lower() for s in lang_pairs[:,0]]\n",
        "lang_pairs[:,1] = [(s.translate(str.maketrans('', '', string.punctuation))).lower() for s in lang_pairs[:,1]]\n",
        "\n",
        "eng_lang_words=lang_pairs[:,0].copy()\n",
        "spa_lang_words=lang_pairs[:,1].copy()\n",
        "\n",
        "eng_lang_words_t=[]\n",
        "spa_lang_words_t=[]\n",
        "\n",
        "\n",
        "# for i in eng_lang_words:\n",
        "#   eng_lang_words_t.append('<start> ' + i + ' <end>')\n",
        "# for i in spa_lang_words:\n",
        "#   spa_lang_words_t.append('<start> ' + i + ' <end>')\n",
        "\n",
        "for i in range(len(eng_lang_words)):\n",
        "  eng_lang_words_t.append(preprocess_sentence(eng_lang_words[i]))\n",
        "\n",
        "for i in range(len(spa_lang_words)):\n",
        "  spa_lang_words_t.append(preprocess_sentence(spa_lang_words[i]))\n",
        "\n",
        "eng_lang_words_t=np.array(eng_lang_words_t)\n",
        "spa_lang_words_t=np.array(spa_lang_words_t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jt9h9jHeucS",
        "colab_type": "code",
        "outputId": "4dfcc57f-a06d-4077-aa9a-51254a724f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "eng_lang_words_t[200]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> push it <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4FCfL91MMAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating dataframe for creating language pairs.\n",
        "li=[eng_lang_words_t,spa_lang_words_t]\n",
        "dataframe=pd.DataFrame(li)\n",
        "dataframe_t=dataframe.T\n",
        "new_lang_pairs=dataframe_t.iloc[:,:].values\n",
        "new_lang_pairs=new_lang_pairs[:100000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzJfwr6kMcle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the tokenization funcntion and padding function.\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_n7g0RlMld1",
        "colab_type": "code",
        "outputId": "928e48ff-9f81-4d8d-848f-d699203d435e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Finding the max length of the sentences in the given language.\n",
        "eng_length=max([len(list(sen)) for sen in new_lang_pairs[:,0]])\n",
        "spa_length=max([len(list(sen)) for sen in new_lang_pairs[:,1]])\n",
        "print(eng_length)\n",
        "print(spa_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55\n",
            "87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwRV-HApMqcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Spilliting the data into training and test set.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(new_lang_pairs, test_size=0.2, random_state = 12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkE037IHMtPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize and encode the sentences.\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(new_lang_pairs[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "# prepare spasian tokenizer\n",
        "spa_tokenizer = tokenization(new_lang_pairs[:, 1])\n",
        "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(spa_tokenizer, spa_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(spa_tokenizer, spa_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia1zX7h9MvuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the layers of the custom Seq to Seq model with Attention\n",
        "class Encoder(tf.keras.models.Model):\n",
        "    def __init__(self,vocab_size,embedding_dim,enc_units,batch_size):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.batch_size=batch_size\n",
        "        self.enc_units=enc_units\n",
        "        self.embedding=Embedding(vocab_size,embedding_dim)\n",
        "        self.gru=GRU(self.enc_units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform') \n",
        "                                                                                                                      \n",
        "    def call(self,x,hidden):\n",
        "        x=self.embedding(x) #Input is feed to embedding which returns word vectors.\n",
        "        output,state=self.gru(x,initial_state=hidden)\n",
        "        return output,state\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size,self.enc_units))\n",
        "\n",
        "#Attention layer\n",
        "class Attention_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self,units):\n",
        "        super(Attention_layer,self).__init__()\n",
        "        self.W1=Dense(units)\n",
        "        self.W2=Dense(units)\n",
        "        self.V=Dense(1)\n",
        "        \n",
        "    def call(self,query,values):\n",
        "        hidden_with_time_axis=tf.expand_dims(query,1)\n",
        "        score=self.V(tf.nn.tanh(self.W1(values)+self.W2(hidden_with_time_axis)))\n",
        "        attention_weights=tf.nn.softmax(score,axis=1)\n",
        "        context_vector=attention_weights*values\n",
        "        context_vector=tf.reduce_sum(context_vector,axis=1)\n",
        "        return context_vector,attention_weights\n",
        "\n",
        "#Decoder.\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,vocab_size,embedding_dim,dec_units,batch_size):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.batch_size=batch_size\n",
        "        self.dec_units=dec_units\n",
        "        self.embedding=Embedding(vocab_size,embedding_dim)\n",
        "        self.gru=GRU(self.dec_units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "        self.fc=Dense(vocab_size)\n",
        "        self.attention=Attention_layer(self.dec_units)\n",
        "        \n",
        "    def call(self,x,hidden,enc_output):\n",
        "        context_vector,attention_weights=self.attention(hidden,enc_output)\n",
        "        x=self.embedding(x)\n",
        "        x=tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output,state=self.gru(x)\n",
        "        output=tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    \n",
        "    \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW1KA2q2M4Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the optimizers and the loss function.\n",
        "optimizer=Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "#Creating objects of encoder,Attention layer and decoder.\n",
        "encoder=Encoder(spa_vocab_size,256,1024,64)\n",
        "decoder=Decoder(eng_vocab_size,256,1024,64)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']] * 64, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss\n",
        "    \n",
        "    \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JvOTU89M_az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAtGW__KNCWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((trainX, trainY)).shuffle(len(trainX))\n",
        "dataset = dataset.batch(64, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb2j9lEHNFTO",
        "colab_type": "code",
        "outputId": "78745fd0-d12d-4d18-f74e-2442337e5fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(len(trainX)//64)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / len(trainX)//64))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.0717\n",
            "Epoch 1 Batch 100 Loss 0.5924\n",
            "Epoch 1 Batch 200 Loss 0.5974\n",
            "Epoch 1 Batch 300 Loss 0.5464\n",
            "Epoch 1 Batch 400 Loss 0.5377\n",
            "Epoch 1 Batch 500 Loss 0.4543\n",
            "Epoch 1 Batch 600 Loss 0.4544\n",
            "Epoch 1 Batch 700 Loss 0.4883\n",
            "Epoch 1 Batch 800 Loss 0.4044\n",
            "Epoch 1 Batch 900 Loss 0.4299\n",
            "Epoch 1 Batch 1000 Loss 0.3789\n",
            "Epoch 1 Batch 1100 Loss 0.3504\n",
            "Epoch 1 Batch 1200 Loss 0.3803\n",
            "Epoch 1 Loss 0.0000\n",
            "Time taken for 1 epoch 785.7883410453796 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.3419\n",
            "Epoch 2 Batch 100 Loss 0.3130\n",
            "Epoch 2 Batch 200 Loss 0.3348\n",
            "Epoch 2 Batch 300 Loss 0.2907\n",
            "Epoch 2 Batch 400 Loss 0.2427\n",
            "Epoch 2 Batch 500 Loss 0.2689\n",
            "Epoch 2 Batch 600 Loss 0.2204\n",
            "Epoch 2 Batch 700 Loss 0.2426\n",
            "Epoch 2 Batch 800 Loss 0.2406\n",
            "Epoch 2 Batch 900 Loss 0.2425\n",
            "Epoch 2 Batch 1000 Loss 0.2096\n",
            "Epoch 2 Batch 1100 Loss 0.1911\n",
            "Epoch 2 Batch 1200 Loss 0.1936\n",
            "Epoch 2 Loss 0.0000\n",
            "Time taken for 1 epoch 737.3531439304352 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1684\n",
            "Epoch 3 Batch 100 Loss 0.1649\n",
            "Epoch 3 Batch 200 Loss 0.1811\n",
            "Epoch 3 Batch 300 Loss 0.1580\n",
            "Epoch 3 Batch 400 Loss 0.1335\n",
            "Epoch 3 Batch 500 Loss 0.1480\n",
            "Epoch 3 Batch 600 Loss 0.1552\n",
            "Epoch 3 Batch 700 Loss 0.1570\n",
            "Epoch 3 Batch 800 Loss 0.1399\n",
            "Epoch 3 Batch 900 Loss 0.1303\n",
            "Epoch 3 Batch 1000 Loss 0.1467\n",
            "Epoch 3 Batch 1100 Loss 0.1400\n",
            "Epoch 3 Batch 1200 Loss 0.1652\n",
            "Epoch 3 Loss 0.0000\n",
            "Time taken for 1 epoch 735.9431807994843 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1091\n",
            "Epoch 4 Batch 100 Loss 0.0902\n",
            "Epoch 4 Batch 200 Loss 0.0849\n",
            "Epoch 4 Batch 300 Loss 0.1107\n",
            "Epoch 4 Batch 400 Loss 0.1144\n",
            "Epoch 4 Batch 500 Loss 0.1118\n",
            "Epoch 4 Batch 600 Loss 0.0968\n",
            "Epoch 4 Batch 700 Loss 0.1307\n",
            "Epoch 4 Batch 800 Loss 0.1087\n",
            "Epoch 4 Batch 900 Loss 0.0902\n",
            "Epoch 4 Batch 1000 Loss 0.1370\n",
            "Epoch 4 Batch 1100 Loss 0.0999\n",
            "Epoch 4 Batch 1200 Loss 0.1024\n",
            "Epoch 4 Loss 0.0000\n",
            "Time taken for 1 epoch 736.5152881145477 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0804\n",
            "Epoch 5 Batch 100 Loss 0.0645\n",
            "Epoch 5 Batch 200 Loss 0.0679\n",
            "Epoch 5 Batch 300 Loss 0.0653\n",
            "Epoch 5 Batch 400 Loss 0.0659\n",
            "Epoch 5 Batch 500 Loss 0.0686\n",
            "Epoch 5 Batch 600 Loss 0.0619\n",
            "Epoch 5 Batch 700 Loss 0.0877\n",
            "Epoch 5 Batch 800 Loss 0.0623\n",
            "Epoch 5 Batch 900 Loss 0.0752\n",
            "Epoch 5 Batch 1000 Loss 0.0742\n",
            "Epoch 5 Batch 1100 Loss 0.0868\n",
            "Epoch 5 Batch 1200 Loss 0.0708\n",
            "Epoch 5 Loss 0.0000\n",
            "Time taken for 1 epoch 735.4758455753326 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0520\n",
            "Epoch 6 Batch 100 Loss 0.0455\n",
            "Epoch 6 Batch 200 Loss 0.0405\n",
            "Epoch 6 Batch 300 Loss 0.0481\n",
            "Epoch 6 Batch 400 Loss 0.0520\n",
            "Epoch 6 Batch 500 Loss 0.0597\n",
            "Epoch 6 Batch 600 Loss 0.0469\n",
            "Epoch 6 Batch 700 Loss 0.0558\n",
            "Epoch 6 Batch 800 Loss 0.0601\n",
            "Epoch 6 Batch 900 Loss 0.0485\n",
            "Epoch 6 Batch 1000 Loss 0.0546\n",
            "Epoch 6 Batch 1100 Loss 0.0498\n",
            "Epoch 6 Batch 1200 Loss 0.0557\n",
            "Epoch 6 Loss 0.0000\n",
            "Time taken for 1 epoch 735.5745885372162 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0308\n",
            "Epoch 7 Batch 100 Loss 0.0357\n",
            "Epoch 7 Batch 200 Loss 0.0315\n",
            "Epoch 7 Batch 300 Loss 0.0357\n",
            "Epoch 7 Batch 400 Loss 0.0332\n",
            "Epoch 7 Batch 500 Loss 0.0402\n",
            "Epoch 7 Batch 600 Loss 0.0347\n",
            "Epoch 7 Batch 700 Loss 0.0458\n",
            "Epoch 7 Batch 800 Loss 0.0410\n",
            "Epoch 7 Batch 900 Loss 0.0452\n",
            "Epoch 7 Batch 1000 Loss 0.0382\n",
            "Epoch 7 Batch 1100 Loss 0.0414\n",
            "Epoch 7 Batch 1200 Loss 0.0384\n",
            "Epoch 7 Loss 0.0000\n",
            "Time taken for 1 epoch 733.7558574676514 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0274\n",
            "Epoch 8 Batch 100 Loss 0.0223\n",
            "Epoch 8 Batch 200 Loss 0.0272\n",
            "Epoch 8 Batch 300 Loss 0.0265\n",
            "Epoch 8 Batch 400 Loss 0.0267\n",
            "Epoch 8 Batch 500 Loss 0.0250\n",
            "Epoch 8 Batch 600 Loss 0.0327\n",
            "Epoch 8 Batch 700 Loss 0.0338\n",
            "Epoch 8 Batch 800 Loss 0.0310\n",
            "Epoch 8 Batch 900 Loss 0.0294\n",
            "Epoch 8 Batch 1000 Loss 0.0279\n",
            "Epoch 8 Batch 1100 Loss 0.0321\n",
            "Epoch 8 Batch 1200 Loss 0.0393\n",
            "Epoch 8 Loss 0.0000\n",
            "Time taken for 1 epoch 736.3017873764038 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0209\n",
            "Epoch 9 Batch 100 Loss 0.0171\n",
            "Epoch 9 Batch 200 Loss 0.0220\n",
            "Epoch 9 Batch 300 Loss 0.0246\n",
            "Epoch 9 Batch 400 Loss 0.0232\n",
            "Epoch 9 Batch 500 Loss 0.0277\n",
            "Epoch 9 Batch 600 Loss 0.0239\n",
            "Epoch 9 Batch 700 Loss 0.0191\n",
            "Epoch 9 Batch 800 Loss 0.0251\n",
            "Epoch 9 Batch 900 Loss 0.0274\n",
            "Epoch 9 Batch 1000 Loss 0.0262\n",
            "Epoch 9 Batch 1100 Loss 0.0234\n",
            "Epoch 9 Batch 1200 Loss 0.0336\n",
            "Epoch 9 Loss 0.0000\n",
            "Time taken for 1 epoch 734.3013904094696 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0200\n",
            "Epoch 10 Batch 100 Loss 0.0236\n",
            "Epoch 10 Batch 200 Loss 0.0218\n",
            "Epoch 10 Batch 300 Loss 0.0205\n",
            "Epoch 10 Batch 400 Loss 0.0235\n",
            "Epoch 10 Batch 500 Loss 0.0262\n",
            "Epoch 10 Batch 600 Loss 0.0219\n",
            "Epoch 10 Batch 700 Loss 0.0252\n",
            "Epoch 10 Batch 800 Loss 0.0223\n",
            "Epoch 10 Batch 900 Loss 0.0276\n",
            "Epoch 10 Batch 1000 Loss 0.0293\n",
            "Epoch 10 Batch 1100 Loss 0.0277\n",
            "Epoch 10 Batch 1200 Loss 0.0238\n",
            "Epoch 10 Loss 0.0000\n",
            "Time taken for 1 epoch 736.3194301128387 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fclPxalc0KCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "\n",
        "  sentence=preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [spa_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=spa_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, 1024))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(spa_length):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "   \n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += eng_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if eng_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "   \n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence\n",
        "\n",
        "def translate(sentence):\n",
        "   result, sentence = evaluate(sentence)\n",
        "\n",
        "   print('Input: %s' % (sentence))\n",
        "   print('Predicted translation: {}'.format(result))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2gbIJKe0Rw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea55a85a-4b7c-42c3-c66b-ffb5adcd106b"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f755c5c90b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHoy3-Dh1YPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a778666b-7a07-442b-e404-41ca6d2d160d"
      },
      "source": [
        "translate(u'él es mi amigo')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> el es mi amigo <end>\n",
            "Predicted translation: hes my friend <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa-1La4s2YaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "65cf9366-a8f2-4b28-98d8-054c4abc3e1c"
      },
      "source": [
        "translate(u'ella es mi novia')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> ella es mi novia <end>\n",
            "Predicted translation: shes my girlfriend <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B37jr5HK70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5d73de29-7de8-4e22-c25a-bac1d9c4311f"
      },
      "source": [
        "translate(u'Le encanta tocar la guitarra')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> le encanta tocar la guitarra <end>\n",
            "Predicted translation: he loves playing playing the guitar <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmM7qqNWHfLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f079a9b0-2903-4778-ef2b-88c6d2335bbe"
      },
      "source": [
        "translate(u'Esto es muy difícil de hacer')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> esto es muy dificil de hacer <end>\n",
            "Predicted translation: this is very difficult to do <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVZ7q2NaH83R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b0325b3a-2fb7-4605-88bd-39b598abcfbe"
      },
      "source": [
        "translate(u'Fue tras el ladrón')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> fue tras el ladron <end>\n",
            "Predicted translation: he was after the thief <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvq3lJXSIUKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1412e443-3c7a-450a-9e46-f6f991e33b40"
      },
      "source": [
        "translate(u'el ingles es un idioma muy divertido')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> el ingles es un idioma muy divertido <end>\n",
            "Predicted translation: english is a very funny language <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiBqq0dG6wIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "d95ef141-3d58-4324-f84e-feda58c8f3dc"
      },
      "source": [
        "!zip -r /content/file.zip /content/training_checkpoints"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/training_checkpoints/ (stored 0%)\n",
            "  adding: content/training_checkpoints/ckpt-2.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-5.data-00000-of-00002 (deflated 16%)\n",
            "  adding: content/training_checkpoints/ckpt-1.data-00001-of-00002 (deflated 15%)\n",
            "  adding: content/training_checkpoints/ckpt-2.data-00001-of-00002 (deflated 14%)\n",
            "  adding: content/training_checkpoints/ckpt-1.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-4.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-3.data-00001-of-00002 (deflated 14%)\n",
            "  adding: content/training_checkpoints/ckpt-5.data-00001-of-00002 (deflated 14%)\n",
            "  adding: content/training_checkpoints/ckpt-2.data-00000-of-00002 (deflated 16%)\n",
            "  adding: content/training_checkpoints/ckpt-5.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-4.data-00000-of-00002 (deflated 16%)\n",
            "  adding: content/training_checkpoints/checkpoint (deflated 38%)\n",
            "  adding: content/training_checkpoints/ckpt-3.data-00000-of-00002 (deflated 16%)\n",
            "  adding: content/training_checkpoints/ckpt-3.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-4.data-00001-of-00002 (deflated 14%)\n",
            "  adding: content/training_checkpoints/ckpt-1.data-00000-of-00002 (deflated 16%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8-7bTCcIhxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/file.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}